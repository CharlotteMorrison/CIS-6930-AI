{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptrons"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, width\n",
    "y = (iris.target == 0).astype(np.int)    # Iris Sestosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Multilayer Perceptrons  \n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "n_inputs = 28*28 # from MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "# use placeholders for the data\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")  # will act as the input layer, during execution it \n",
    "                                                                  # will be replaced with one training batch at a time.\n",
    "y = tf.placeholder(tf.int64, shape=None , name =\"y\")\n",
    "\n",
    "# create 2 hidden layers and the output layer\n",
    "\n",
    "# tensorflow has a standard neural network\n",
    "\n",
    "with tf.name_scope(\"s_dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='outputs')\n",
    "    \n",
    "# after defining the model, next define the cost function to train it, here using cross entropy\n",
    "# cross entropy will penalize models with a low probability for the target class.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# define a gradient descent optimizer that will tweak the model parameters to minimize the cost function\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# model evaluation, here use accuracy basically test if the models logit is the same as the target class\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# create a node to initialize all variables and create a saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Execution phase: load MNIST from TensorFlow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# define the number of epochs and batch sizes\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# train the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,  y: mnist.validation.labels})\n",
    "        print(epoch, \" Train accuracy: \", acc_train, \" Val accuracy: \", acc_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    X_new_scaled = [...]  # some new images (sclaed from 0 to 1)\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-Tuning Neural Netwokr Hyperparameters\n",
    "\n",
    "### Number of Hidden Layers\n",
    "One hidden layer can model even the most complex functions if it has enough neurons.<br/>\n",
    "But deep networks have higher parameter efficiency: they need less neurons to model, making them faster<br/>\n",
    "You can use a neural network trained on a more general problem as the lower levels for a more specific problem.<br/>\n",
    "For most problems start with 1 or 2 hiddeen layers, then ramp up the number of levels once you determine if you are \n",
    "getting a good solution or not.  \n",
    "\n",
    "### Number of Neurons Per Hidden Layer\n",
    "The input layer is determined by the size of the dataset, the output is determined by the size of the output (how many\n",
    "classes for the decision).  The neurons should form a funnel with the input feeding into the largest layer.  The number\n",
    "of neurons can be increased gradually until overfitting.  It is generally better to increase the number of layers than\n",
    "the number of neurons within a layer.  Stretch pants method: choose one larger than you need then stop early to prevent \n",
    "overfitting.\n",
    "\n",
    "### Activation Function\n",
    "ReLU is the most common for the hidden layers because it is a bit faster and Gradient Descent doesn't tend to get stuck \n",
    "on plateaus.  Softmax is preferred for classification tasks output layer where the tasks are mutually exclusive.  If \n",
    "they are not mutually exclusive (or binary) then the logistic function is preferred.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}