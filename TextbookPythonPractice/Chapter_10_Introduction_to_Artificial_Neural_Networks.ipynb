{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, width\n",
    "y = (iris.target == 0).astype(np.int)    # Iris Sestosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Multilayer Perceptrons  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1006 12:13:33.654783 140311386486528 deprecation.py:323] From <ipython-input-2-207e79ca645e>:22: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1006 12:13:33.664160 140311386486528 deprecation.py:506] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1006 12:13:35.274155 140311386486528 deprecation.py:323] From <ipython-input-2-207e79ca645e>:50: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1006 12:13:35.275295 140311386486528 deprecation.py:323] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1006 12:13:35.276549 140311386486528 deprecation.py:323] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 12:13:35.596879 140311386486528 deprecation.py:323] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1006 12:13:35.652189 140311386486528 deprecation.py:323] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0  Train accuracy:  0.94  Val accuracy:  0.901\n",
      "1  Train accuracy:  0.94  Val accuracy:  0.9226\n",
      "2  Train accuracy:  0.94  Val accuracy:  0.9318\n",
      "3  Train accuracy:  0.94  Val accuracy:  0.9396\n",
      "4  Train accuracy:  0.96  Val accuracy:  0.9468\n",
      "5  Train accuracy:  0.98  Val accuracy:  0.9498\n",
      "6  Train accuracy:  0.9  Val accuracy:  0.9532\n",
      "7  Train accuracy:  0.98  Val accuracy:  0.9566\n",
      "8  Train accuracy:  0.96  Val accuracy:  0.9626\n",
      "9  Train accuracy:  1.0  Val accuracy:  0.9634\n",
      "10  Train accuracy:  0.98  Val accuracy:  0.9658\n",
      "11  Train accuracy:  0.98  Val accuracy:  0.964\n",
      "12  Train accuracy:  0.96  Val accuracy:  0.9684\n",
      "13  Train accuracy:  1.0  Val accuracy:  0.9696\n",
      "14  Train accuracy:  1.0  Val accuracy:  0.9706\n",
      "15  Train accuracy:  0.96  Val accuracy:  0.972\n",
      "16  Train accuracy:  1.0  Val accuracy:  0.9726\n",
      "17  Train accuracy:  1.0  Val accuracy:  0.9736\n",
      "18  Train accuracy:  0.98  Val accuracy:  0.9738\n",
      "19  Train accuracy:  1.0  Val accuracy:  0.974\n",
      "20  Train accuracy:  0.96  Val accuracy:  0.975\n",
      "21  Train accuracy:  0.98  Val accuracy:  0.9762\n",
      "22  Train accuracy:  1.0  Val accuracy:  0.976\n",
      "23  Train accuracy:  1.0  Val accuracy:  0.9764\n",
      "24  Train accuracy:  0.96  Val accuracy:  0.9764\n",
      "25  Train accuracy:  1.0  Val accuracy:  0.9764\n",
      "26  Train accuracy:  0.96  Val accuracy:  0.976\n",
      "27  Train accuracy:  0.96  Val accuracy:  0.9764\n",
      "28  Train accuracy:  0.98  Val accuracy:  0.9766\n",
      "29  Train accuracy:  0.98  Val accuracy:  0.9776\n",
      "30  Train accuracy:  1.0  Val accuracy:  0.9772\n",
      "31  Train accuracy:  0.98  Val accuracy:  0.979\n",
      "32  Train accuracy:  1.0  Val accuracy:  0.9784\n",
      "33  Train accuracy:  1.0  Val accuracy:  0.9776\n",
      "34  Train accuracy:  1.0  Val accuracy:  0.9786\n",
      "35  Train accuracy:  1.0  Val accuracy:  0.9794\n",
      "36  Train accuracy:  1.0  Val accuracy:  0.9796\n",
      "37  Train accuracy:  1.0  Val accuracy:  0.98\n",
      "38  Train accuracy:  1.0  Val accuracy:  0.9788\n",
      "39  Train accuracy:  1.0  Val accuracy:  0.9808\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "n_inputs = 28*28 # from MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "# use placeholders for the data\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")  # will act as the input layer, during execution it \n",
    "                                                                  # will be replaced with one training batch at a time.\n",
    "y = tf.placeholder(tf.int64, shape=None , name =\"y\")\n",
    "\n",
    "# create 2 hidden layers and the output layer\n",
    "\n",
    "# tensorflow has a standard neural network\n",
    "\n",
    "with tf.name_scope(\"s_dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='outputs')\n",
    "    \n",
    "# after defining the model, next define the cost function to train it, here using cross entropy\n",
    "# cross entropy will penalize models with a low probability for the target class.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# define a gradient descent optimizer that will tweak the model parameters to minimize the cost function\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# model evaluation, here use accuracy basically test if the models logit is the same as the target class\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# create a node to initialize all variables and create a saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Execution phase: load MNIST from TensorFlow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# define the number of epochs and batch sizes\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# train the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,  y: mnist.validation.labels})\n",
    "        print(epoch, \" Train accuracy: \", acc_train, \" Val accuracy: \", acc_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final_10.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 12:16:47.716015 140311386486528 deprecation.py:323] From /home/charlotte/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: ./my_model_final.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e9968a0a382b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./my_model_final.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX_new_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# some new images (sclaed from 0 to 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_new_scaled\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CIS-6930-AI/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[0;32m-> 1278\u001b[0;31m                        compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: ./my_model_final.ckpt"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    X_new_scaled = [...]  # some new images (sclaed from 0 to 1)\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Netwokr Hyperparameters\n",
    "\n",
    "### Number of Hidden Layers\n",
    "One hidden layer can model even the most complex functions if it has enough neurons.<br/>\n",
    "But deep networks have higher parameter efficiency: they need less neurons to model, making them faster<br/>\n",
    "You can use a neural network trained on a more general problem as the lower levels for a more specific problem.<br/>\n",
    "For most problems start with 1 or 2 hiddeen layers, then ramp up the number of levels once you determine if you are \n",
    "getting a good solution or not.  \n",
    "\n",
    "### Number of Neurons Per Hidden Layer\n",
    "The input layer is determined by the size of the dataset, the output is determined by the size of the output (how many\n",
    "classes for the decision).  The neurons should form a funnel with the input feeding into the largest layer.  The number\n",
    "of neurons can be increased gradually until overfitting.  It is generally better to increase the number of layers than\n",
    "the number of neurons within a layer.  Stretch pants method: choose one larger than you need then stop early to prevent \n",
    "overfitting.\n",
    "\n",
    "### Activation Function\n",
    "ReLU is the most common for the hidden layers because it is a bit faster and Gradient Descent doesn't tend to get stuck \n",
    "on plateaus.  Softmax is preferred for classification tasks output layer where the tasks are mutually exclusive.  If \n",
    "they are not mutually exclusive (or binary) then the logistic function is preferred.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
