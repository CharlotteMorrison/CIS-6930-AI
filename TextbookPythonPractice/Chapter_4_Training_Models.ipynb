{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression: two methods of training.<br/>\n",
    "* using a direct closed-form equation that directly computes the model parameters that best fit the model to the training set.\n",
    "* using an iterative optimization technique, Gradient Descent (GD), gradually tweaks the model parameters to minimize the cost\n",
    "function over the training set, converges to the same set of parameters at the first method.<br/>\n",
    "Polynomial Regression: can fit non-linear datasets.<br/> "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the linear model makes a prediction by computing the weighted sum of the input features, plus a constant  called the\n",
    "# bias term, aka intercept term\n",
    "# For linear regression find parameter vector that minimizes the MSE (easier than RMSE) and gives same value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Normal Equation\n",
    "To find the value of theta that minimizes the cost function- there is a closed form solution called the Normal Equation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate some linear data to work with.\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)\n",
    "\n",
    "# compute the inverse of a matrix\n",
    "X_b = np.c_[np.ones((100,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ideal results would be [4,3] but the added noise makes it impossible to get the exact parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make predictions using theta hat\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  #  add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform a linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg.predict(X_new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# linear regression is based on least squares, that can be called directly\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the pseudoinverse directly\n",
    "np.linalg.pinv(X_b).dot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computational Complexity\n",
    "Linear Regress class is O(n^2)\n",
    "\n",
    "### Gradient Descent\n",
    "Generic optimization algorithm- finds optimal solutions fo a range of problems<br/>\n",
    "This is done by tweaking parameters iteratively.<br/>\n",
    "The learning rate hyperparameter: start with random numbers then improve gradually. If the learning rate is too small,\n",
    "it will take to many iterations to converge, if it is too large, you may jump past the optimal solution and never get a\n",
    "solution.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "To compute the gradient descent- compute the gradient of the cost function with regards to each model parameter.<br/>\n",
    "This is basically calculating the cost function if you change the model parameter a little (partial derivative). <br/>\n",
    "These don't need to be calculated individually- they can be done at one time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gradient descent implementation\n",
    "\n",
    "# if the learning rate is low, it will take a long time, if too high, it jumps all over the place\n",
    "# a grid search can be used to find the optimal learning rates (limit iterations so long convergence times are eliminated)\n",
    "# interrupt the test when the gradient vector becomes tiny (not much is gained)\n",
    "\n",
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Stochastic Gradient Descent\n",
    "Batch gradient descent uses the whole training set for each step, making it slow for large datasets.  SGD picks a random\n",
    "instance at each step and computes gradients on only those sets.  This makes this faster and workable for huge datasets.<br/>\n",
    "It is much less "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}