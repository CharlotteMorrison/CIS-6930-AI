{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Deep Neural Nets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanishing/Exploding Gradients Problems\n",
    "### Vanishing gradient problem\n",
    "Because gradients sometimes get smaller as they progress down the network, the weights may become too small to converge \n",
    "on a solution.\n",
    "### Exploding Gradient problem\n",
    "In some cases, they may become larger as they progress through the network causing the algorithm to diverge.<br/>\n",
    "\n",
    "This means that deep neural networks have unstable gradients, layers may learn at very different speeds.  This has \n",
    "been show by analyzing the networks.  The use of the logistic sigmoid activation causes the outputs to have a \n",
    "higher variance than the inputs, this increases as we work through the network.  The function then saturates near\n",
    "the boundaries of 0 and 1 causing the backpropagation to have almost no gradient to propagate through the layers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Xavier and He Initialization\n",
    "To deal with the data flowing in both directions: forward for making predictions, and backwards when propagating \n",
    "gradients.  The problems of saturation and explosion must be dealt with.  This means that the variance of the inputs\n",
    "and outputs should be equal going both ways through the network.  The only way to ensure this is to have the same\n",
    "amount of neurons, but that is not possible.  \n",
    "#### Possible solution:\n",
    "The connection weights should be initialized randomly at each layer.  THis speeds up training and has led to the \n",
    "success of deep learning.  tf.layers.dense() uses Xavier initialization, to use He, see code snip below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import tensorflow as tf\n",
    "X = {}\n",
    "n_hidden = 100  # dummy values\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(factor=2.)\n",
    "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, kernal_initializer=he_init, name=\"hidden1\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Nonsaturating Activation Functions\n",
    "Part of the cause of the exploding/vanishing problem is caused by the choice of activation function.<br/>\n",
    "ReLU does not saturate for positive values, so it is often chosen for a deep network.  But it can cause\n",
    "some neurons to die, they only output zero.  If you use a large learning rate, this might be particularly bad.\n",
    "### Leaky ReLU\n",
    "This allows the function to 'leak' over time using a hyperparameter &alpha;.  This keeps the neurons from dying over time, they\n",
    "may go into a 'coma' but they can recover from this state.  From research &aplha; = 0.2 is more effective than \n",
    "&alpha; = 0.01.<br/>\n",
    "### ELU versus ReLU\n",
    "Exponential Linear Unit (ELU) can outperform ReLU.  It is similar to ReLU, but differs because it takes on \n",
    "negative values so the average is closer to 0.  This helps with the vanishing gradients problem.  It has a non-zero \n",
    "gradient for z is less than 0 that prevents dying.  If &alpha; is equal to one, it is smooth everywhere speeding \n",
    "up gradient descent (doesn't bounce past the minimum).  But it is slower to compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Batch Normalization\n",
    "Batch normalization (BN) is used to solve the vanishing and exploding gradients problem (that the distribution of each\n",
    "layers inputs changes during training).  In BN an operation is added before the activation function of each layer. \n",
    "This just centers the data on zero and normalizes it, then scales and shifts the data. The model learns the optimal \n",
    "scale and mean for each layer inputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Implementing Batch Normalization with Tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")  # will act as the input layer, during execution it \n",
    "                                                                  # will be replaced with one training batch at a time.\n",
    "y = tf.placeholder(tf.int64, shape=None , name =\"y\")\n",
    "training = tf.Variable(False, shape=(), name='training')\n",
    "\n",
    "# creates a wrapper around the function and defines the defaults for some parameters\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "# the rest of the the program is the same as chapter 10\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# define a gradient descent optimizer that will tweak the model parameters to minimize the cost function\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# model evaluation, here use accuracy basically test if the models logit is the same as the target class\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# create a node to initialize all variables and create a saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Execution phase: load MNIST from TensorFlow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# define the number of epochs and batch sizes\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# train the model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "        print(epoch, \" Test Accuracy: \", acc_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final_11.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Gradient Clipping\n",
    "To solve exploding gradients- the gradients can be clipped during backpropagation so they don't exceed a threshold.\n",
    "Used mostly in neural network, but batch normalization tends to be preferred.  In tensorflow the minimise() function \n",
    "handles the clipping, after the gradients have been computed.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Reusing Pretrained Layers\n",
    "You can reuse parts of neural networks that have been already trained to speed up your new task.  If the model was trained\n",
    "in tensorflow you can restore it with import_meta_graph() function to get the default graph (will have a .meta extension).  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reuse the model trained above\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final_11.ckpt.meta\")\n",
    "\n",
    "# Figure out the tensors and the operations needed for testing.\n",
    "# the name of the tensor is the name of the operation that ouput it followed by :0, for first, :1 second, and so on.\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n",
    "\n",
    "# if not well documented  explore the graph using TensorBoard\n",
    "for op in tf.get_default_graph().get_operation():\n",
    "    print(op.name)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# could also create a collection with all the important operations\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)\n",
    "    \n",
    "# can be easily reused by\n",
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}