{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "Fits the widest possible lane between clusters of data. (large margin classification)<br/>\n",
    "Adding more instances will not affect the decision boundary, because it is fully supported by the \n",
    "instances located on the edge of the lanes (support vectors).\n",
    "\n",
    "## Soft Margin Classification\n",
    "Hard margin classification requires that all the data be classified on sides of the lane.  This requires the data\n",
    "to be linearly separable and it is very affected by outliers.  Outliers can make it impossible to find a hard margin\n",
    "because they may appear on the wrong side of the lane.<br/>\n",
    "Soft margin classification keeps the lane as large as possible but also minimizes the margin violation. \n",
    "### Scikit-Learn\n",
    "Control the balance using the C hyperparameter.  Smaller c leads to a wider lane, but more margin violations, and a\n",
    "larger c will give you a smaller lane, with fewer margin violations. To counter overfitting, regularize by reducing c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)\n",
    "\n",
    "svm_clf = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))])\n",
    "\n",
    "svm_clf.fit(X, y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "svm_clf.predict([[5.5, 1.7]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nonlinear SVM Classification\n",
    "When datasets are not linearly separable: can add polynomial features.<br/>\n",
    "Scikit-learn has a polynomial features transformer for the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=100, shuffle=True, noise=0.1, random_state=42)\n",
    "\n",
    "X = X_moons[:,0]\n",
    "y = X_moons[:,1]\n",
    "plt.scatter(X, y)\n",
    "plt.show()\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polynomial_svm_clf.fit(X_moons, y_moons)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Polynomial Kernel\n",
    "It is easy to add polynomial features- low number of polynomial degree cannot deal with complexity, high number\n",
    "will make the model slow.<br/>\n",
    "\n",
    "### Kernel Trick\n",
    "Get the same result as adding many polynomial features (without actually doing it). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Kernel trick on the moon dataset\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))  # coef0 controls how much the model is influenced by\n",
    "    # high degree polynomials versus low-degree.\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X_moons, y_moons)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding Similarity Features\n",
    "To handle non-linear problems you can add features computed using a similarity function.  The similarity function\n",
    "measures how much each instance resembles a landmark.  It is a bell shaped function from 0 to 1, with larger number\n",
    "indicating closer to the landmark.<br/>\n",
    "To create a landmark, you can set one for each data point in the dataset- this is not the best for larger datasets.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian RBF Kernel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}