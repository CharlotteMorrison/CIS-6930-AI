# Ensemble Learning: Chapter 7
Hard Voting: To create a better classifier, aggregate the predictions of each classifier and predict the class that gets the most votes.<br/>
Weak learner: barely better than random guessing.<br/>
An ensembles predictions will be better than the best individual, even works when using multiple weak learners (if there is a sufficient number of learners.<br/>
Ensemble works best when the predictors are independent.<br/>

## Soft Voting Predictions
THe classifiers rely on probabilities for soft voting.<br/>
The ensemble will predict the class with the highest class probability.<br/>

## Bagging and Pasting
* Bagging: sampling with replacement (could be duplicates)
* Pasting: sampling without replacement

## Random Forests
An ensemble of decision trees using bagging.<br>
To ensure diversity, the alg searches for the best feature among a random set of attributes.

## Boosting
Learning method for ensemble learning that uses the same dataset.  The first time, pick a model,
let it classify, then next time, look at the misclassified data.  Then the algorithm forces the focus to those that 
are misclassified by putting more weight on those, or use gradient boost (later).    A second classifier is trained, then
the misclassified are identified- this continues until a perfect model is found or a number of times have run.
  