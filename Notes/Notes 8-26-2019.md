# Notes 8/26/2019

### Regularization
* to cure overfiting, apply a small penalty.
* divide data in test/train by 80/20 or 70/30 depending on needs

### Decision Trees
Connected, acyclic graphs are trees. <br>
Have nodes and edges, nodes have decisions- edges have solutions (ie. true/false)<br>
Essentially giving a bunch of rules to classify objects.<br>
These give a good white box model, easy to interpret.<br>
#### Downsides to decision trees:
Prone to overfitting (root is very powerful, can be restrictive), this can be regularized by using multiple trees.  Two 
trees might each overfit by themselves, but they may provide good predictions when they are combined.
#### Bagging = Bootstrap Aggregation
Use the same training algorithm for every predictor but train them on different random subsets.
#### Boosting 
Train several weak learner sequential, each trying to correct the errors made by its predecessor.<br>
Adaptive boosting can give more relative weight to the misclassified instances. (improving one single model)