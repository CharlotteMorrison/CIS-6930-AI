# Artificial Neural Networks Chapter 10
### Project Notes
Implement the algorithm and data structure, not using the sklearn built in functions. <br/>
Can use ndarrays, sets, data structures...<br/>
Sigmoid function is often called the logit function 1/(1+e^-x)

## Artificial neuron: linear threshold unit (LTU)
With one or more numeric inputs, it produces a weighed sum of them, applies and activation function and outputs the  
result.

Common activation functions: step function and sigmoid function

Sigmoid function- benefit: easy to compute, differentiable, continuous, Y is bounded by 0 and 1

Every propositional statement can be represented by a neural network.

## Perceptron
Two layers of neurons, an ipu layer consisting of special passing through neurons and an output layer of LTU's<br/>
The bias neuron is added for completeness of linearity.  It fills out the missing constant.<br/>
If training examples are linearly separable a perceptron always can be learned to correctly classify all training 
examples.<br/> Limited to only linearly solvable problems.  They cannot solve some trivial non-linear separable problems
like XOR, can do logical or, and or negation.<br/>

## Multi-Layer Perceptron (MLP), Deep Neural Networks (DNN)
Composed of one passthrough input layer, one or more layers of LTU's, called hidden layer and one final layer of LTUs.
Stacking perceptrons into multi-layers (hidden layers) can solve non-linear problems.<br/>
Slide 8/17 question will potentially be on the quiz, practice calculating the XOR.<br/>
When an MLP has more than one hidden layer it is a deep neural network.<br/>

## Learning ANN's: Error Backpropagation
Idea: we start with a fixed network, then update edge weights using  V <-- V + delta V.<br/>
There is no theory on how many neurons are optimal for a problem.  There is a rule of thumb: start with one hidden 
layer, the number of layers is more important than the neurons.  Work through with trial-and -error for the 
hyperparameters.  We are focused in this lecture on fully connected networks.

### Activation functions and derivatives.
The bias neuron is gon from the picture, the bias is embedded in the LTU's activation function (theta).  We are learning 
weights and biases.  <br/>
Use softmax for class probabilities.
Step function is not great because no gradient descent, reLU=max{x,0} is easy to compute but there are non-differential
areas (0) and is thought to be close to neurons and learning rate can be faster, Tanh s-shaped but bounded by {-1, 1}, 
sigmoid is differentiable everywhere though but can take a long time to converge with a small learning rate.<br/>
The error function is a composition function of many parameters and is differentiable (convex graph) so we can compute 
the gradient descents to be use to update the weights and biases.  We want to minimize the error.
The learning rate is a value from 0-1, start with 0.1.  Indicates how far you go in between steps.  If too small, you 
won't miss the bottom of the function, but it could be very slow.  If the learning rate is too large, you may jump
past the minimum.<br/>

### Algorithm for error backpropogation
* Initialize the weights and biases using random values.
* Set up the number of epochs (how many times to run)
* for every (x,y) in the training set
    * for each x, calculate y-hat
    * calculate gradient descent deltas for the neurons isn the output layer
    * calculate gradient descent deltas for neurons in the hidden layers
    * Updates weights and biases