## Gradient Descent
Learning rate: the size of the descent step, gradient steps change the step size
as it moves downward to reach the global minimum.
* Batch Gradient Descent: Use the whole training set.
* Stochastic Gradient Descent: randomly pick one value to calculate the gradients.
* Mini-Batch Gradient Descent: uses a small set of the values.

### Underfitting
To cure underfitting do not add more samples, use a more complex mode.

### Overfitting
More examples may help, also regularization.

## Ridge Regression
Regularization is to constrain a model.  If it has less freedom it makes it harder to overfit.
In linear regression constraining is done by constraining the thetas (weights).<br/>
The alpha hyperparameter: bigger results in flatter and smoother model.

## Lasso Regression
The second way to constrain the weights, least absolute shrinkage and selection operator.
It adds the L1 norm, not the L2 in ridge to the cost function.<br>
Lasso tends to completely remove the weights of the least important features by setting them 
to zero, automatically performs feature selection.

## Elastic Net 
Combination of Ridge and Lasso.<br/>
Use Lasso or Elastic if you think attributes are not very important.<br/>
If you have many more features than training examples: elastic (something like a picture with 
a million pixels)<br/>


## Logistic Regression
Outputs class probabilities for binary regression problems, which can be used to predict classes for binary 
classification problems. <em>For multi-class regression use Softmax.<em/><br/>
First we design a cost function for one single training example.
For positive examples, we want the cost function to be small when the
probability is big; for negative examples, when the probability is small.  Logistic functions
do not solve for multi-class problems.

## Softmax Regression
Idea: given an instance x, we first compute a score sk (x) for each
class k, then estimate the probability of each class by applying the
softmax function.
